%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{url}
\usepackage{xcolor}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\oa}[1]{\footnote{\color{red}OA: #1}}
\newcommand{\oamod}[1]{{\color{red}#1}}
\newcommand{\lc}[1]{\footnote{\color{blue}LC: #1}}
\newcommand{\lcmod}[1]{{\color{blue}#1}}
\newcommand{\daniel}[1]{\footnote{\color{brown}DH: #1}}
\newcommand{\daneldad}[1]{\footnote{\color{green}DE: #1}}


\title{The Language of Legal and Illegal Activity on DarkNet}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
  The non-indexed parts of the Internet (the DarkNet)\daniel{I think it should be Darknet (in this capitalization) and always with ``the'' preceding it}
   have become a haven for both legal and illegal annonymized activity.
  Given the magnitude of these networks, scalably monitoring activity necessarily relies
    on automated tools, and notably on NLP tools.
  However, little is known about what characteristics texts communicated through DarkNet have, 
    and how well do off-the-shelf NLP tools do on this domain.
  This paper tackles this gap and performs an in-depth investigation of the characteristics
    of legal and illegal text in DarkNet, comparing it to a clear net website with similar
    content as a control condition.
  Taking drugs-related website as a test case, we find that texts for selling legal and illegal drugs
    have several linguistic charateristics that distinguish them from one another, as well as from 
    the control condition, among them the distribution of POS tags, and the coverage of their named entities in Wikipedia.
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
  
  Under the cloak of anonimity, the Darknet is a home to much illegal activity \citep{moore2016cryptopolitik}.
  Applying NLP tools to text from the DarkNet is thus important for effective law enforcement and intelligence.
  However, little is known about the characteristics of the language used in the DarkNet, 
  and specifically on what distinguishes text on websites that conduct legal and illegal activity.
  
  \citet{AlNabki17} were the first to crawl webpages from Onion sites (non-index sites commonly accessed using the Tor browser), 
  and used it to perform text classification into topical categories (e.g., ``drugs'', ``services'' etc.).
  Classifying texts into legal and illegal activity over text from Tor was first pursued by \citet{Avarikioti18}.
  However, to our knowledge no previous work attempted to reveal in what linguistic dimensions do sites that advertise
  legal and illegal activities differ. 
  
  This paper addresses this gap, and studies the distinguishing features between legal and illegal texts in Onion sites,
  taking sites that advertise drugs as a test case. We compare our results to a control condition of texts 
  from eBay\footnote{\url{https://www.ebay.com/}} pages that advertise drugs
  (supposedly of the pharmaceutical kind). 
  
  We find a number of distinguishing features. First, we confirm the results of \citet{Avarikioti18}, that text from legal and illegal
  pages can be distinguished based on the identity of the content words (bag of words model) 
  in about 90\% accuracy over a balanced sample. Second, we find that the distribution of POS tags in the documents is a strong cue for distinguishing
  legal and illegal pages (about 75\% accuracy). This indicates that the texts of the two types of websites are different in terms of their syntactic 
  structure. Third, we find that legal and illegal texts are roughly as distinguisable from one another as legal texts and eBay pages (both in terms
  of their words and the distinguishability of the distribution of their POS tags). This shows that legal and illegal texts can be considered
  distinct domains, which on the one hand implies that they can be distinguished, but on the other hand suggests that applying NLP tools to this
  domain is likely to face the obstacles of domain adaptations.  
  Fourth, we show that named entities in illegal pages are covered less well by Wikipedia, i.e., Wikification works less well on them.
  This suggests that for proper text understanding, specialized knowledge bases and tools may be needed for pages on illegal drugs, 
    which underscores the pitfalls of na\"{i}vely applying off-the-shelf NLP tools to this domain.
  
  
  
%   
%    published a corpus and looked into text classification on texts from Tor, 
%   
%   
%   and Y et al. (2019) looked into how influential criminal sites are. 
%
%   While this domain has attracted much attention, both in the academia
%
%   %-- We investigate how legal and illegal activity taken from DarkNet is different, comparing to a clearnet website with similar content as a control condition.
%
%   We also explore methods for classifying texts from DarkNet into legal and illegal. This is important both for understanding whether these two types are different in terms of
%   their text and in what ways, and as a practical tool.
%
%   A bit on the experiments and results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

Elior

%\paragraph{Detecting illegal activities in the Web}

The detection of illegal activities in the Web is sometimes derived from a more general topic classification. For example, \citet{Biryukov14} used the software Mallet \citep{McCallum02} and the web service uClassify \citep{Kagstrom13} for a classification of the content of Tor hidden services into 18 categories, which allows the distinction between illegal or contreversial content on one hand and human rights or freedom of speech content on the other hand. \citet{GraczykKinningham15} combined unsupervised feature selection and an SVM classfier for the classification of drug sales in an anonymous marketplace. However, although the detection of illegal activities can be easily deduced in some cases, the legal status of a given product can change \citep{GraczykKinningham15} and a given topic could cover both legal and illegal content. For example, in the recent work of \citet{Avarikioti18} on Tor content clasification, in most of the categories both legal and illegal content appear.

Some works have directly addressed a specific type of illegality and a particular communication context. \citet{MorrisHirst12} have used an SVM classification to identify sexual predators in chatting message systems. The model includes both lexical features, including emoticons and behavioral features that correspond to conversational patterns. Another example is the detection of pedophile activity in peer-to-peeer networks \citep{Latapy13} where a predefined list of keywords was used to detect child-pornography queries.

\citet{AlNabki17} presented the DUTA (Darknet Usage Text Addresses), the first publicly available Darknet dataset, together with a manual classification into classes and sub-classes. For some of the classes, legal and illegal activities are distinguished. However, the automatic classification presented in their work focuses on the differentiation between several illegal activities, without dealing with the separation between legal and illegal activities, which is the subject of the present paper. \citet{AlNabki19} further extended the DUTA-10K dataset, which we are using here, and report that 20\% of the hidden services correspond to suspiscious activities. This analysis is done using the text classifier presented in \citet{AlNabki17} and a manual verification, where the information is directly derived from a categorization into classes.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datasets}\label{sec:data}

DUTA \citep{AlNabki17}

\paragraph{Cleaning.} Elior + Daniel

As preprocessing for all experiments, we apply some cleaning to the text
of web pages in our corpus.
HTML markup is already removed in the original dataset,
but many nonlinguistic content remains, such as
buttons, encryption keys, metadata and URLs.
We remove such text from the web pages, and also join paragraphs to single lines
(as newlines are sometimes present in the original dataset for display purposes
only).
We then remove any duplicate paragraphs, where paragraphs are considered
identical if they share all but numbers
(to avoid an over-representation of some remaining surrounding text from the
websites, e.g. ``Showing all 9 results Search for
...'').\footnote{Our preprocessing code is included in the supplementary
material, and will be publicly available upon publication.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Domain Analysis}

\subsection{Distances between Domains}
\begin{table}[]
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{@{}lllll@{}}
			& all onion  & eBay       & illegal    & legal      \\
			all onion &            & 0.60, 1.31 & 0.33, 0.62 & 0.35, 0.65 \\
			eBay      & 0.60, 1.31 &            & 0.59, 1.28 & 0.66, 1.46 \\
			illegal   & 0.33, 0.62 & 0.59, 1.28 &            & 0.61, 1.28 \\
			legal     & 0.35, 0.65 & 0.66, 1.46 & 0.61, 1.28 &           
		\end{tabular}%
	}
	\caption{Jensen-Shannon divergence and Variational distance between word distribution in all onion drug sites, legal and illegal onion drug sites, and eBay sites. \label{ta:domain}}
\end{table}
\begin{table*}[]
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{@{}lllllllllllll@{}}
			& all onion  & all onion\_half1 & all onion\_half2 & ebay       & ebay\_half1 & ebay\_half2 & illegal    & illegal\_half1 & illegal\_half2 & legal      & legal\_half1 & legal\_half2 \\
			all onion        &            & 0.23, 0.34       & 0.25, 0.38       & 0.60, 1.31 & 0.61, 1.35  & 0.61, 1.35  & 0.33, 0.62 & 0.39, 0.77     & 0.41, 0.81     & 0.35, 0.65 & 0.41, 0.82   & 0.42, 0.82   \\
			all onion\_half1 & 0.23, 0.34 &                  & 0.43, 0.73       & 0.60, 1.32 & 0.62, 1.35  & 0.62, 1.35  & 0.37, 0.63 & 0.33, 0.63     & 0.50, 0.96     & 0.40, 0.70 & 0.36, 0.70   & 0.52, 1.00   \\
			all onion\_half2 & 0.25, 0.38 & 0.43, 0.73       &                  & 0.61, 1.34 & 0.62, 1.36  & 0.62, 1.37  & 0.39, 0.69 & 0.50, 0.96     & 0.35, 0.67     & 0.39, 0.67 & 0.51, 0.99   & 0.35, 0.66   \\
			ebay             & 0.60, 1.31 & 0.60, 1.32       & 0.61, 1.34       &            & 0.23, 0.36  & 0.25, 0.39  & 0.59, 1.28 & 0.60, 1.30     & 0.60, 1.32     & 0.66, 1.46 & 0.67, 1.48   & 0.67, 1.49   \\
			ebay\_half1      & 0.61, 1.35 & 0.62, 1.35       & 0.62, 1.36       & 0.23, 0.36 &             & 0.43, 0.74  & 0.60, 1.32 & 0.61, 1.33     & 0.61, 1.34     & 0.67, 1.48 & 0.67, 1.48   & 0.68, 1.50   \\
			ebay\_half2      & 0.61, 1.35 & 0.62, 1.35       & 0.62, 1.37       & 0.25, 0.39 & 0.43, 0.74  &             & 0.60, 1.30 & 0.61, 1.32     & 0.61, 1.32     & 0.67, 1.49 & 0.68, 1.50   & 0.68, 1.50   \\
			illegal          & 0.33, 0.62 & 0.37, 0.63       & 0.39, 0.69       & 0.59, 1.28 & 0.60, 1.32  & 0.60, 1.30  &            & 0.23, 0.35     & 0.27, 0.42     & 0.61, 1.28 & 0.62, 1.31   & 0.62, 1.31   \\
			illegal\_half1   & 0.39, 0.77 & 0.33, 0.63       & 0.50, 0.96       & 0.60, 1.30 & 0.61, 1.33  & 0.61, 1.32  & 0.23, 0.35 &                & 0.45, 0.77     & 0.62, 1.31 & 0.63, 1.33   & 0.62, 1.32   \\
			illegal\_half2   & 0.41, 0.81 & 0.50, 0.96       & 0.35, 0.67       & 0.60, 1.32 & 0.61, 1.34  & 0.61, 1.32  & 0.27, 0.42 & 0.45, 0.77     &                & 0.62, 1.31 & 0.63, 1.33   & 0.63, 1.33   \\
			legal            & 0.35, 0.65 & 0.40, 0.70       & 0.39, 0.67       & 0.66, 1.46 & 0.67, 1.48  & 0.67, 1.49  & 0.61, 1.28 & 0.62, 1.31     & 0.62, 1.31     &            & 0.26, 0.40   & 0.26, 0.42   \\
			legal\_half1     & 0.41, 0.82 & 0.36, 0.70       & 0.51, 0.99       & 0.67, 1.48 & 0.67, 1.48  & 0.68, 1.50  & 0.62, 1.31 & 0.63, 1.33     & 0.63, 1.33     & 0.26, 0.40 &              & 0.47, 0.82   \\
			legal\_half2     & 0.42, 0.82 & 0.52, 1.00       & 0.35, 0.66       & 0.67, 1.49 & 0.68, 1.50  & 0.68, 1.50  & 0.62, 1.31 & 0.62, 1.32     & 0.63, 1.33     & 0.26, 0.42 & 0.47, 0.82   &             
		\end{tabular}%
	}
	\caption{Jensen-Shannon divergence and Variational distance between word distribution in all onion drug sites, legal and illegal onion drug sites, and eBay sites.
	Each domain was also split in half for within-domain comparison. \label{ta:domain_halves}}
	
\end{table*}
At this point, we strive to know which of the domains we deal with is a different domain in terms of word use, whether all onion language should be considered the same, and is there a potential in learning differences between onion sites. For those means, we created a histogram of the frequencies of words in the eBay corpus and the legal and illegal drugs onion corpora, together with the combination of the two latter, representing all drug sites in onion. Additionally, each corpus was randomly split in two halves allowing for in-domain comparison. Following \citet{Plank2011EffectiveMO} we chose Jensen-Shannon divergence and Variational distance (also known as L1 or Manhattan) as the comparison measures between the word frequency histograms.\daniel{Leshem: do we really need \textit{both} measures?}

We see in Table \ref{ta:domain_halves} that eBay, legal and illegal are self-distant 0.4-0.45 but the distance between one another is 0.6-0.65. This means the three form an equilateral triangle. We can therefore think of the three as three different domains. The results suggest there is no reason to think about all onion as one domain, and thus studying using onion data what characterizes the illegal domain from the legal domain is sensible.

We also see that eBay is as far from the legal domain as it is from the illegal or for that case all onion domain. This would suggest there are also unique characteristics for each domain which will make learning illegal properties rather than simply domain differences more challenging. For this reason if one wishes to understand differences that stem from legality issues, it might prove more beneficial to differentiate illegal and legal inside the onion domain, rather than world wide web legal domains such as eBay.

\subsection{NER and Wikification}

In order to analyze the named entities in each domain and the differences
between them we used a ``wikification'' technique, searching for
said entities in public datasets such as Wikipedia. 

Using SpaCy's\footnote{\url{https://spacy.io}}
named entity recognition, we first found all the named
entities in each site in the datasets. After finding the named entites
we searched for relevant Wikipedia entries for each named entity using
the DBpedia Ontology API \footnote{\url{https://www.dbpedia-spotlight.org/}}.
For each domain we counted the total number
of named entites and what percentage of them had corresponding Wikipedia
articles.

\begin{table}
\begin{center}
\begin{tabular}{lr}
 & \% Wikifiable\\
Legal Onion & 50.8 $\pm2.31$\\
Illegal Onion & 32.5 $\pm1.35$\\
eBay & 38.6 $\pm2$\\
\end{tabular}
\end{center}
\caption{Percentage of wikifiable named entites per domain.\label{ta:wiki}}
\end{table}
According to our results (Table~\ref{ta:wiki}) the wikification percentages of
eBay sites and illegal Onion sites are comparable and relatively low.
However, sites selling legal drugs on Onion have a much higher wikification
percentage.

Presumably the named entities in Onion sites selling legal drugs are
more easily found in public databases such as Wikipedia because they
are mainly well known names for legal pharmaceuticals. However, in
both illegal Onion and eBay sites, the list of named entities include
many slang terms for illicit drugs and paraphernalia. These slang terms
are usually not well known by the general public and are therefore
less likely to be found on Wikipedia or other public databases.

In addition to the differences in wikification percentages between
the domains, we found that SpaCy had trouble correctly identifying
named entities in both Onion and eBay sites. There were a fair number
of false positives (words and phrases that were found by SpaCy but
were not actually named entities),\daniel{Dan: is this observation
quantifiable? Can you tell what ratio are false positives? If not,
mention that this is just our impression and perhaps give examples}
especially in illegal Onion sites. In particular, nicknames for drugs as well as initialed drugs, for example "kush" or "GBL", were being falsely picked up by SpaCy's NER.
It should be noted that that ratio of false positives is not something that is easily quantifiable, and therefore this is only a qualitative observation. That being said, it is our impression that the informal language used in our datasets makes it harder
for SpaCy to function optimally in this capacity.\daneldad{cleared up a bit and added examples as per daniel's comment}

These findings lead us to believe that the popular tools used today
for named entity recognition and analysis are not ideal for processing
informal language on the internet, especially language dealing in
illicit activities. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classification Experiments}

Here we detail our experiments in classifying text from different legal and
illegal domains using various methods, to find the most important features
distinguishing between the domains.

\paragraph{Experimental setup.}

After cleaning the dataset, joining lines to paragraphs and removing duplicates
(see \S\ref{sec:data}), we split each subset into training, validation and test.
We select 456 training paragraphs, 57 validation paragraphs and
58 test paragraphs (approximately a 80\%/10\%/10\% split) for each category,
thus randomly downsampling larger categories for an even division of labels.

\paragraph{Model.}

To classify paragraphs into categories, we experiment with five classifiers:

\begin{itemize}
  \item Naive Bayes: we use \texttt{BernoulliNB} from
  \texttt{scikit-learn}\footnote{\url{https://scikit-learn.org}}
  with $\alpha=1$.
  The features are BoW (bag-of-words), i.e., indicator feature for each word.
  \item SVM: we use \texttt{SVC}, also from \texttt{scikit-learn},
  with $\gamma=$``scale'' and tolerance=$10^{-5}$.
  We use BoW features in this case too.
  \item BoE (bag-of-embeddings): we represent each word with its 100-dimensional
  GloVe vector \cite{pennington2014glove}, average the embeddings for all words in the paragraph
  to a single vector, and apply a 100-dimensional fully-connected layer with
  ReLU non-linearity and dropout $p=0.2$.
  The word vectors are not updated during training.
  \item seq2vec: same as BoE, but instead of averaging word vectors,
  we apply a BiLSTM to the word vectors, and take the concatenated
  final hidden vectors from the forward and backward part as the input to the
  fully-connected layer.
  \item ELMo+attention: we replace the word representations with contextualized
  pre-trained represetations from ELMo \cite{Peters:2018}. We then apply a self-attentive
  classification network \cite{mccann2017learned} over the contextualized representations. This architecture proved very accurate for classification in
  previous work \cite{W18-5427,D18-1401}.
\end{itemize}

We use the AllenNLP Python library\footnote{\url{https://allennlp.org}}
\cite{Gardner2017AllenNLP} for implementing the neural network classifiers.

\paragraph{Data manipulation.}

To check the sensitivity of classifiers to variations in content words,
function words and syntax, we experiment with five manipulations to the input
data (in training, validation and testing).
For this purpose, we consider content words as words whose universal part-of-speech
according to spaCy is one of \{ADJ, ADV, NOUN, PROPN, VERB, X, NUM\},
and function words as all other words.
The tested manipulations are:

\begin{itemize}
  \item Dropping all content words.
  \item Dropping all function words.
  \item Replacing all content words with their universal part-of-speech.
  \item Replacing all function words with their universal part-of-speech.
  \item Replacing all words with their universal part-of-speech.
\end{itemize}

\subsection{Results}

\paragraph{Legal vs. Illegal}

\paragraph{Legal Onion vs. eBay}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

The legal and illegal are pretty distant, which is evident in a few ways: word distribution is different, NER and Wikification work less well
for illegal. This has practical implications: we need to adapt our tools to deal with illegal Tor data. 

Looking at specific sentences, we see that it's hard distinguishing them based on the identity of the words, which means that
looking at the wordforms is a very poor solution for tackling this. However, using modern text classification, they can be distinguished
in a 72\%. 

Looking at how different types of language influence results: given that replacing all words with their POS tags gives
the same performance, this tells us their syntax is different as well.

Methological: Legal and illegal in Onion are distinct enough to be considered different domains (as distant as Legal and eBay). 
Therefore, Tor could be a good testbed for working on legal and illegal classification.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
 












\bibliography{acl2019}
\bibliographystyle{acl_natbib}




\end{document}
