%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper,table]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{url}
\usepackage{xcolor}
\usepackage{pgf}
\usepackage{collcell}
%\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\oa}[1]{\footnote{\color{red}OA: #1}}
\newcommand{\oamod}[1]{{\color{red}#1}}
\newcommand{\lc}[1]{\footnote{\color{blue}LC: #1}}
\newcommand{\lcmod}[1]{{\color{blue}#1}}
\newcommand{\daniel}[1]{\footnote{\color{brown}DH: #1}}
\newcommand{\daneldad}[1]{\footnote{\color{green}DE: #1}}
\newcommand{\es}[1]{\footnote{\color{purple}ES: #1}}


% for results matrix
\newcommand{\ApplyGradient}[1]{%
  \pgfmathsetmacro{\PercentColor}{ifthenelse(#1-0 > 0, (#1-21)*.8, 0)}%
  \pgfmathsetmacro{\PercentInverse}{ifthenelse(\PercentColor > 70, 0, 100)}%
  %\textcolor{black!\PercentColor}{#1}
  \edef\x{\noexpand\cellcolor{red!\PercentColor}}\x\textcolor{black!\PercentInverse}{#1}%
}
\newcolumntype{R}{>{\collectcell\ApplyGradient}{c}<{\endcollectcell}}


\title{The Language of Legal and Illegal Activity on the Darknet}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
  The non-indexed parts of the Internet (the Darknet)
   have become a haven for both legal and illegal anonymous activity.
  Given the magnitude of these networks, scalably monitoring their activity necessarily relies
    on automated tools, and notably on NLP tools.
  However, little is known about what characteristics texts communicated through the Darknet have, 
    and how well do off-the-shelf NLP tools do on this domain.
  This paper tackles this gap and performs an in-depth investigation of the characteristics
    of legal and illegal text in the Darknet, comparing it to a clear net website with similar
    content as a control condition.
  Taking drugs-related websites as a test case, we find that texts for selling legal and illegal drugs
    have several linguistic characteristics that distinguish them from one another, as well as from 
    the control condition, among them the distribution of POS tags, and the coverage of their named entities in Wikipedia.
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

  The Darknet refers to the subset of Internet sites and pages that are not
  indexed by search engines.
  Due to the their use of the ``.onion'' top-level domain, these websites are
  often referred to as ``Onion sites'', and are reachable via the Tor network
  anonymously.
  
  Under the cloak of anonymity, the Darknet harbors much illegal activity \citep{moore2016cryptopolitik}.
  Applying NLP tools to text from the Darknet is thus important for effective law enforcement and intelligence.
  However, little is known about the characteristics of the language used in the Darknet, 
  and specifically on what distinguishes text on websites that conduct legal and illegal activity.
	The only work we are aware of that classified Darknet texts into legal and illegal activity is \citet{Avarikioti18}.
	However, they too did not investigate in what ways these two classes differ.
  
  
  %Text classification of crawled Darknet pages was pursued by .
	%\citet{AlNabki17} pursued text classification of webpages from 
	%Onion (non-indexed sites commonly accessed using the Tor browser), 
	%and performend classification into topical categories (e.g., ``drugs'', ``services'' etc.). %\es{I think \citet{moore2016cryptopolitik} were the first to use crawling for Tor but they only collected 5K samples of Tor Onion pages for the topic classification and there didn't publish an annotated corpus.}
  This paper addresses this gap, and studies the distinguishing features between legal and illegal texts in Onion sites,
  taking sites that advertise drugs as a test case. We compare our results to a control condition of texts 
  from eBay\footnote{\url{https://www.ebay.com/}} pages that 
	advertise products corresponding to drug keywords (see Section \ref{sec:datasets}).
 
  We find a number of distinguishing features. First, we confirm the results of \citet{Avarikioti18}, 
	that text from legal and illegal pages can be distinguished based on the identity of the content words (bag of words) 
  in about 90\% accuracy over a balanced sample. Second, we find that the distribution of POS tags in the documents is a strong cue for 
	distinguishing between the classes (about 75\% accuracy). This indicates that the two classes are different in 
	terms of their syntactic structure. Third, we find that legal and illegal texts are roughly as distinguishable from one another as legal 
	texts and eBay pages (both in terms of their words and their POS tags). 
	The latter point suggests that legal and illegal texts can be considered distinct domains, which explains why they can be 
	automatically classified, but also implies that applying NLP tools to this domain is likely to face the obstacles of domain adaptation.  
  Indeed, we show that named entities in illegal pages are covered less well by Wikipedia, i.e., Wikification works less well on them.
  This suggests that for high-performance text understanding, specialized knowledge bases and tools may be needed for processing texts from the Darknet.
  % Capitalization should be Darknet, not DarkNet
  
    %which underscores the pitfalls of na\"{i}vely applying off-the-shelf NLP tools to this domain.
  
	
%   
%    published a corpus and looked into text classification on texts from Tor, 
%   
%   
%   and Y et al. (2019) looked into how influential criminal sites are. 
%
%   While this domain has attracted much attention, both in the academia
%
%   %-- We investigate how legal and illegal activity taken from Darknet is different, comparing to a clearnet website with similar content as a control condition.
%
%   We also explore methods for classifying texts from Darknet into legal and illegal. This is important both for understanding whether these two types are different in terms of
%   their text and in what ways, and as a practical tool.
%
%   A bit on the experiments and results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

The detection of illegal activities in the Web is sometimes derived from a more general topic classification. For example, \citet{Biryukov14}
%used the software Mallet \citep{McCallum02}\oa{you mean LDA features? if so, better state the model than the implementation}\es{they don't give details about the model}and the web service uClassify \citep{Kagstrom13}
classified the content of Tor hidden services into 18 categories, allowing the distinction between topics that usually correlate with illegal activity and those that usually do not. \citet{GraczykKinningham15} combined unsupervised feature selection and an SVM classifier for the classification of drug sales in an anonymous marketplace.
These works classified Tor texts into classes, that can give information as to whether the products are legal or legal, but did not address the legal/illegal classification.

%However, although the detection of illegal activities can be easily deduced in some cases, the legal status of a given product can change \citep{GraczykKinningham15} and a given topic could cover both legal and illegal content. These works classified Tor texts into classes, that can give information as to whether the products are legal or legal, but did not address the legal/illegal classification.
%For example, in the recent work of \citet{Avarikioti18} on Tor content classification, in most of the categories both legal and illegal content appear. We here explore the distinction between legal and illegal activities within a given topical category.\oa{I think we can greatly simplify this parag, simply saying that these works classified Tor texts into classes, that can give information as to whether the products are legal or legal, but did not address the legal/illegal classification}

Some works directly addressed a specific type of illegality and a particular communication context. \citet{MorrisHirst12} used an SVM classification to identify sexual predators in chatting message systems. The model includes both lexical features, including emoticons and behavioral features that correspond to conversational patterns. Another example is the detection of pedophile activity in peer-to-peer networks \citep{Latapy13}, where a predefined list of keywords was used to detect child-pornography queries. In addition to lexical features, we here consider other general linguistic properties, such as syntactic structure.% whose analysis can be extended to a wide range of legal/illegal cases. 

\citet{AlNabki17} presented DUTA (Darknet Usage Text Addresses), the first publicly available Darknet dataset, together with a manual classification into topical categories and sub-categories. For some of the categories, legal and illegal activities are distinguished. However, the automatic classification presented in their work focuses on the distinction between several illegal activities, without dealing with the separation between legal and illegal ones, which is the subject of the present paper. \citet{AlNabki19} extended the dataset to form the DUTA-10K, which we are using here. Their results show that 20\% of the hidden services correspond to ``suspicious'' activities. This analysis is done using the text classifier presented in \citet{AlNabki17} and a manual verification. %where the information is directly derived from the topic classifier.\oa{this last part of the sent is unclear}

Recently, \citet{Avarikioti18} presented another topic classification of text from Tor together with a first classification into legal and illegal activities.  The experiments were performed on a newly crawled corpus obtained by a recursive search. The legal/illegal classification was done using an SVM classifier in an active learning setting with bag of words as features. Legality was assessed in a conservative way where illegality is assigned if the purpose of the content is an obvioulsy illegal action, even if the content might be technically legal. They found that a linear kernel worked best and reported an F1 score of 85\% and an accuracy of 89\%. Using the dataset of \citet{AlNabki19}, and focusing on a given topical category, we here confirm the importance of content words in the classification and explore the linguistic dimensions permitting the legal/illegal distinction. 




%    \begin{table}[]
%    \resizebox{\columnwidth}{!}{%
%        \begin{tabular}{@{}lllll@{}}
%& all onion & eBay & illegal & legal      \\
%all onion && 0.60, 1.31 & 0.33, 0.62 & 0.35, 0.65 \\
%eBay & 0.60, 1.31 && 0.59, 1.28 & 0.66, 1.46 \\
%illegal & 0.33, 0.62 & 0.59, 1.28 && 0.61, 1.28 \\
%legal & 0.35, 0.65 & 0.66, 1.46 & 0.61, 1.28 &     
%        \end{tabular}%
%    }
%    \caption{Jensen-Shannon divergence and Variational distance between word distribution in all Onion drug sites, legal and Illegal Onion drug sites, and eBay sites. \label{ta:domain}}
%\end{table}
\begin{table*}[]
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}ll|rrr|rrr|rrr|rrr@{}}
&& \multicolumn{3}{c|}{All Onion} & \multicolumn{3}{c|}{eBay}
& \multicolumn{3}{c|}{Illegal Onion} & \multicolumn{3}{c}{Legal Onion} \\
&& all & half 1 & half 2 & all & half 1 & half 2
& all & half 1 & half 2 & all & half 1 & half 2 \\
\hline
& all & & 0.23 & 0.25 & 0.60 & 0.61 & 0.61 & 0.33 & 0.39 & 0.41 & 0.35 & 0.41 & 0.42 \\
All Onion & half 1 & 0.23 & & 0.43 & 0.60 & 0.62 & 0.62 & 0.37 & 0.33 & 0.50& 0.40 & 0.36 & 0.52 \\
& half 2 & 0.25 & 0.43 & & 0.61 & 0.62 & 0.62 & 0.39 & 0.50& 0.35 & 0.39 & 0.51 & 0.35 \\
\hline
& all & 0.60 & 0.60 & 0.61 & & 0.23 & 0.25 & 0.59 & 0.60& 0.60& 0.66 & 0.67 & 0.67 \\
eBay & half 1 & 0.61 & 0.62 & 0.62 & 0.23 & & 0.43 & 0.60 & 0.61 & 0.61 & 0.67 & 0.67 & 0.68 \\
& half 2 & 0.61 & 0.62 & 0.62 & 0.25 & 0.43 & & 0.60 & 0.61 & 0.61 & 0.67 & 0.68 & 0.68 \\
\hline
& all & 0.33 & 0.37 & 0.39 & 0.59 & 0.60 & 0.60 & & 0.23 & 0.27 & 0.61 & 0.62 & 0.62 \\
Illegal Onion & half 1 & 0.39 & 0.33 & 0.50 & 0.60 & 0.61 & 0.61 & 0.23 & & 0.45 & 0.62 & 0.63 & 0.62 \\
& half 2 & 0.41 & 0.50 & 0.35 & 0.60 & 0.61 & 0.61 & 0.27 & 0.45 & & 0.62 & 0.63 & 0.63 \\
\hline
& all & 0.35 & 0.40 & 0.39 & 0.66 & 0.67 & 0.67 & 0.61 & 0.62 & 0.62 & & 0.26 & 0.26 \\
Legal onion & half 1 & 0.41 & 0.36 & 0.51 & 0.67 & 0.67 & 0.68 & 0.62 & 0.63 & 0.63 & 0.26 & & 0.47 \\
& half 2 & 0.42 & 0.52 & 0.35 & 0.67 & 0.68 & 0.68 & 0.62 & 0.62 & 0.63 & 0.26 & 0.47 &  \\          
        \end{tabular}%
    }
    \caption{Jensen-Shannon divergence between word distribution in all Onion drug sites, Legal and Illegal Onion drug sites, and eBay sites.
    Each domain was also split in half for within-domain comparison. \label{ta:domain_halves}}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datasets Used}\label{sec:datasets}

\paragraph{Onion corpus.}
We experiment with data from Darknet websites concerning
legal and illegal activity, all from the DUTA-10K corpus \citep{AlNabki19}.
We selected the ``drugs'' sub-domain as a test case, considering both ``legal drugs'' and ``illegal drugs'' sub-categories.
%\begin{description}
%\item[Legal and illegal drugs.]
These websites advertise and sell
drugs, often to international customers.
While legal websites often sell pharmaceuticals,
illegal ones are often related to substance abuse.
These pages are written by sellers and directed to their customers.
  
%\end{description}

\paragraph{eBay corpus.}
As an additional dataset of similar size and characteristics,
but from a clear net source, and of legal nature,
we created a corpus of eBay pages.
eBay is one of the largest hosting sites for retail sellers of various goods. Our corpus contains 118 item descriptions, each consisting of more than a few words. Those item descriptions vary in price, item sold and seller. The descriptions were chosen by searching eBay for drug related terms,\footnote{Namely,  \textit{marijuana}, \textit{weed}, \textit{grass} and \textit{drug}.} and selecting search patterns to avoid over-repetition of the mentioned factors.\daniel{This sentence requires clarification.} Either because of advertisement strategies or the origins of the sellers (from all over the world), the eBay corpus contains formal as well as informal language, and some item descriptions contain abbreviations and slang.
Importantly, eBay websites are assumed to conduct legal activity---even
if discussing drug-related material, we find it is never the sale of illegal
drugs but rather merchandise, tools, or otherwise related content.

\paragraph{Cleaning.} 
As preprocessing for all experiments, we apply some cleaning to the text
of web pages in our corpora.
HTML markup is already removed in the original datasets,
but much nonlinguistic content remains, such as
buttons, encryption keys, metadata and URLs.
We remove such text from the web pages, and join paragraphs 
to single lines (as newlines are sometimes present in the original dataset for display purposes only).
We then remove any duplicate paragraphs, where paragraphs are considered
identical if they share all but numbers (to avoid an over-representation of some remaining surrounding text from the websites, e.g. ``Showing all 9 results'').\footnote{Our data and preprocessing code is included in the supplementary material, and will be made publicly available upon publication.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Domain Differences}\label{sec:domain}
    
    We turn to quantifying the domain difference between text from legal and illegal texts. This question bears on the possibility of distinguishing between legal and illegal drug-related websites based on their text alone (i.e., without recourse to additional information, such as meta-data or network structure).

\subsection{Vocabulary Differences}

    To quantify the domain differences between texts from legal and illegal texts,
    we compute the frequency distribution of words in the eBay corpus, the legal and illegal drugs Onion corpora, and the entire Onion drug section (All~Onion). 
    %We combine the latter two, representing all drug sites in onion. 
    Since any two sets of texts are bound to show some disparity between them, we compare the differences between domains to a control setting, where we randomly split each examined corpus into two halves, and compute the frequency distribution of the two halves.
    The inner consistency of each corpus, defined as the similarity of distribution between the two halves, serves as a reference point for the similarity between domains.
    We refer to this measure as ``self-distance''.
    
    Following \citet{Plank2011EffectiveMO}, we compute the Jensen-Shannon divergence and Variational distance (also known as L1 or Manhattan) as the comparison measures between the word frequency histograms.
    We see in Table~\ref{ta:domain_halves} that the self-distance of the
    eBay, Legal Onion and Illegal Onion corpora lies between 0.40 to 0.45
    by the Jensen-Shannon divergence, but the distance between each pair
    is 0.60 to 0.65.
    This means the three approximately form an equilateral triangle. 
    The results suggest there is no reason to think about all Onion as one domain, and thus studying using Onion data what characterizes the illegal domain from the legal domain is sensible.\oa{I didn't get the last part of the sentence; why is this entailed?}
    Similar results were obtained using Variational distance, and are omitted for brevity.

    We also see that eBay is as far from the Legal Onion domain as it is from the Illegal Onion domain, and from All Onion.
    This suggests that there are also unique characteristics for each domain, which make learning properties of Illegal Onion more challenging
    than simply domain differences.
    For this reason if one wishes to understand differences that stem from legality issues, it might prove more beneficial to differentiate illegal and legal inside the Onion domain,
    rather than world wide web legal domains such as eBay.\oa{didn't understand the last parag. can we omit it?}

\subsection{Differences in Named Entities}

    In order to analyze the difference in the distribution of 
    named entities between the domains,  we used a ``wikification''\cite{bunescu2006using} technique, i.e., linking entities to their corresponding article in Wikipedia.

    Using spaCy's\footnote{\url{https://spacy.io}}
    named entity recognition, we first extract all named
    entity mentions from all the corpora. 
    We then search for relevant Wikipedia entries for each named entity using the DBpedia Ontology API \cite{isem2013daiber}.
    For each domain we compute the total number of named entites and what percentage of them had corresponding Wikipedia articles.

\begin{table}
\begin{center}
\begin{tabular}{l|r}
 & \% Wikifiable\\
 \hline
eBay & $38.6 \pm2.00$\\
Illegal Onion & $32.5 \pm1.35$\\
Legal Onion & $50.8 \pm2.31$
\end{tabular}
\end{center}
\caption{Average percentage of wikifiable named entities in a website per domain, with standard error.\label{ta:wiki}}
\end{table}

The results (shown in Table~\ref{ta:wiki}) were obtained by averaging the percentage of wikifiable named entities in each site per domain. We also calculated the standard error for each average.

According to our results, the wikification success ratios of eBay and Illegal Onion named entities is comparable and relatively low. However, sites selling legal drugs on Onion have a much higher wikification percentage.

Presumably the named entities in Onion sites selling legal drugs are
more easily found in public databases such as Wikipedia because they
are mainly well-known names for legal pharmaceuticals. However, in
both Illegal Onion and eBay sites, the list of named entities includes
many slang terms for illicit drugs and paraphernalia. These slang terms
are usually not well known by the general public, and are therefore
less likely to be covered by Wikipedia and similar public databases.


In addition to the differences in wikification ratios between
the domains, it seems spaCy had trouble correctly identifying
named entities in both Onion and eBay sites, possibly
due to the commmon use of informal language and drug-related jargon.
Eyeballing the results, there were a fair number of false positives (words and phrases that were found by spaCy but were not actually named entities),
especially in Illegal Onion sites.
In particular, slang terms for drugs, as well as abbreviated drug terms, for example ``kush'' or ``GBL'',
were being falsely picked up by spaCy's NER.
We consider these to be false positives as they are names of substances but not specific products, and hence not named entities.\footnote{See \url{https://spacy.io/api/annotation\#section-named-entities}}

 The results in these experiments suggest both that (1) legal and illegal activity is different in terms of its named entities and their coverage in Wikipedia, as well as that (2) standard NLP tools for named entity recognition (and potentially other text understanding tasks), and standard databases, require considerable adaptation to be fully functional on text related to illegal activity. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classification Experiments} \label{sec:classification}

Here we detail our experiments in classifying text from different legal and
illegal domains using various methods, to find the most important linguistic features
distinguishing between the domains. Another goal of the classification task is to confirm our finding that the domains are distinguishable, assuming different domains should be separable by a classifier.

\paragraph{Experimental setup.}

After cleaning the dataset, joining lines to paragraphs and removing duplicates
(see \S\ref{sec:datasets}), we split each subset into training, validation and test.
We select 456 training paragraphs, 57 validation paragraphs and
58 test paragraphs (approximately a 80\%/10\%/10\% split) for each category,
thus randomly downsampling larger categories for an even division of labels.

\paragraph{Model.}

To classify paragraphs into categories, we experiment with five classifiers:

\begin{itemize}
  \item NB (Naive Bayes) classifier
  with BoW (bag-of-words) features, i.e., indicator feature for each word.
  \item SVM (support vector machine) classifier with an RBF kernel,
  also with BoW features.
  \item BoE (bag-of-embeddings): we represent each word with its 100-dimensional
  GloVe vector \cite{pennington2014glove}, average (or sum) the embeddings for all words in the paragraph
  to a single vector, and apply a 100-dimensional fully-connected layer with
  ReLU non-linearity and dropout $p=0.2$.
  The word vectors are not updated during training.
  \item seq2vec: same as BoE, but instead of averaging word vectors,
  we apply a BiLSTM to the word vectors, and take the concatenated
  final hidden vectors from the forward and backward part as the input to the
  fully-connected layer.
  \item attention: we replace the word representations with contextualized
  pre-trained representations from ELMo \cite{Peters:2018}. We then apply a self-attentive
  classification network \cite{mccann2017learned} over the contextualized representations. This architecture proved very accurate for classification in
  previous work \cite{W18-5427,D18-1401}.
\end{itemize}

For the NB classifier we use \texttt{BernoulliNB} from
\texttt{scikit-learn}\footnote{\url{https://scikit-learn.org}}
with $\alpha=1$,
and for the SVM clasifier we use \texttt{SVC}, also from \texttt{scikit-learn},
with $\gamma=\mathrm{scale}$ and tolerance=$10^{-5}$.
We use the AllenNLP Python library\footnote{\url{https://allennlp.org}}
\cite{Gardner2017AllenNLP} for implementing the neural network classifiers.

\paragraph{Data manipulation.}

To check the sensitivity of classifiers to variations in content words,
function words and syntax, we experiment with four manipulations to the input
data (in training, validation and testing).
For this purpose, we consider content words as words whose universal part-of-speech
according to spaCy is one of the following:
\[\{\mathrm{ADJ, ADV, NOUN, PROPN, VERB, X, NUM}\}\]
and function words as all other words.
The tested manipulations are:

\begin{itemize}
  \item Dropping all content words (drop cont.).
  \item Dropping all function words (drop func.).
  \item Replacing all content words with their universal part-of-speech (pos cont.).
  \item Replacing all function words with their universal part-of-speech (pos func.).
\end{itemize}

\paragraph{Settings.}

We experiment with four settings, classifying paragraphs from different domains:
\begin{itemize}
  \item Training and testing on eBay pages vs. Legal Onion pages,
  as a control experiment
  to identify whether Onion pages differ from clear net pages.
  \item Training and testing on Legal Onion vs. Illegal Onion drugs-related pages,
  to identify the difference in language between legal and illegal activity
  on Onion drug-related websites.
  
\end{itemize}

\subsection{Results} \label{subsec:results}

\begin{table}[t]
\centering
\setlength\tabcolsep{4pt}
%\def\arraystretch{.855}
\begin{tabular}{l *{5}{R}}
&& \multicolumn{1}{c}{\bf drop} & \multicolumn{1}{c}{\bf drop} & \multicolumn{1}{c}{\bf pos} & \multicolumn{1}{c}{\bf pos}\\
& \multicolumn{1}{c}{\bf full} & \multicolumn{1}{c}{\bf cont.} & \multicolumn{1}{c}{\bf func} & \multicolumn{1}{c}{\bf cont.} & \multicolumn{1}{c}{\bf func}\\
\multicolumn{6}{l}{\bf eBay vs. Legal Onion Drugs} \\
\hline
NB & 91.4 & 57.8 & 90.5 & 56.9 & 92.2\\
SVM & 63.8 & 64.7 & 63.8 & 68.1 & 63.8\\
BoE$_\mathrm{sum}$ & 66.4 & 56.0 & 63.8 & 50.9 & 76.7\\
BoE$_\mathrm{average}$ & 75.0 & 55.2 & 59.5 & 50.0 & 75.0\\
seq2vec & 73.3 & 53.8 & 65.5 & 65.5 & 75.0\\
attention & 82.8 & 57.5 & 85.3 & 62.1 & 82.8\\
\\
\multicolumn{6}{l}{\bf Legal Onion vs. Illegal Drugs} \\
\hline
NB & 77.6 & 53.4 & 87.9 & 51.7 & 77.6\\
SVM & 63.8 & 66.4 & 63.8 & 70.7 & 63.8\\
BoE$_\mathrm{sum}$ & 52.6 & 61.2 & 74.1 & 50.9 & 51.7\\
BoE$_\mathrm{average}$ & 57.8 & 57.8 & 52.6 & 55.2 & 50.9\\
seq2vec & 56.9 & 55.0 & 54.3 & 59.5 & 49.1\\
attention & 64.7 & 51.4 & 62.9 & 55.2 & 69.0
\end{tabular}
\caption{Test accuracy for each classifier (rows) in each setting (columns) on drugs-related data.
\label{tab:results_drugs}}
\end{table}

\begin{table}[t]
\centering
\setlength\tabcolsep{4pt}
%\def\arraystretch{.855}
\begin{tabular}{l *{5}{R}}
&& \multicolumn{1}{c}{\bf drop} & \multicolumn{1}{c}{\bf drop} & \multicolumn{1}{c}{\bf pos} & \multicolumn{1}{c}{\bf pos}\\
& \multicolumn{1}{c}{\bf full} & \multicolumn{1}{c}{\bf cont.} & \multicolumn{1}{c}{\bf func} & \multicolumn{1}{c}{\bf cont.} & \multicolumn{1}{c}{\bf func}\\
\multicolumn{6}{l}{\bf Legal Onion vs. Illegal Forums} \\
\hline
NB & 74.1 & 50.9 & 78.4 & 50.9 & 72.4\\
SVM & 85.3 & 75.9 & 56.0 & 81.9 & 81.0\\
BoE$_\mathrm{sum}$ & 25.9 & 32.8 & 21.6 & 36.2 & 35.3\\
BoE$_\mathrm{average}$ & 40.5 & 42.2 & 31.9 & 48.3 & 53.4\\
seq2vec & 50.0 & 48.9 & 50.9 & 28.4 & 51.7\\
attention & 31.0 & 37.2 & 33.6 & 27.6 & 30.2\\
\\
\multicolumn{6}{l}{\bf Trained on Drugs, Tested on Forums} \\
\hline
NB & 78.4 & 63.8 & 89.7 & 63.8 & 79.3\\
SVM & 62.1 & 69.0 & 54.3 & 69.8 & 62.1\\
BoE$_\mathrm{sum}$ & 45.7 & 50.9 & 49.1 & 50.9 & 50.0\\
BoE$_\mathrm{average}$ & 49.1 & 51.7 & 51.7 & 52.6 & 58.6\\
seq2vec & 51.7 & 61.1 & 51.7 & 54.3 & 57.8\\
attention & 65.5 & 59.2 & 65.5 & 50.9 & 66.4
\end{tabular}
\caption{Test accuracy for each classifier (rows) in each setting (columns) on forums data.
\label{tab:results_forums}}
\end{table}


\paragraph{Legal Onion vs. eBay.}

This control experiment shows that Legal Onion content is quite easily
distinguishable from eBay content, as a Naive Bayes bag-of-words classifier
reaches 91.4\% accuracy on this classification.
Moreover, replacing function words by their parts of speech even improves
the performance, suggesting that the content is mostly useful for determining
whether a page comes from Onion or eBay.
This is confirmed by the drop in accuracy when content words are dropped:
however, in this setting (drop cont.) the SVM classifier performed best,
suggesting there is a non-linear relationship between the function words
found in a page and its source.
While other, more sophisticated classifiers performed worse, the attention
classifier managed to get as close as possible to the simple Naive Bayes
one, as it has pre-trained contextualized word embeddings to compensate for
the small amount of training data.

\paragraph{Legal vs. illegal drugs.}

Classifying legal from illegal pages within the drugs domain on Onion
proved to be a more difficult task.
In this setting, content proved to be even more important:
dropping function words entirely improved the performance of the Naive Bayes
classifier to nearly its accuracy in the previous experiment.
This suggests that syntactic structure is similar between the legal and illegal
domains, but their content allows differentiating them, e.g.,
by the terms they use to describe their products. 

\section{Illegality Detection Across Domains}

\subsection{Experimental setup}

To investigate illegality detection across different domains, %we replicate the legal/illegal classification experiment, using
we perform classification experiments on the  ``forums'' category that is also separated into legal and illegal sub-categories in DUTA-10K.
The forums  contain user-written text in various topics. Legal forums often discuss web design and other technical
and non-technical activity on the internet, while illegal ones involve
discussions about cyber-crimes and guides on how to commit them,
as well as narcotics, racism and other criminal activities.
As this domain contains user-generated content, it is more varied
and noisy. We use the cleaning process described in \ref{sec:datasets} and data splitting described in \ref{sec:classification}, with the same number of paragraphs.
%In a second experiment, we experiment on the forums test set with the classifiers trained on the legal/illegal drugs data to check to which extent illegality detection can be applied across different domains.

We experiment with two settings:
\begin{itemize}
\item Training and testing on Onion legal vs. illegal forums,
to evaluate the insights observed in the drugs domain generalize to user-generated content.
\item Training on Onion legal vs. illegal drugs-related pages,
and testing on Onion legal vs. illegal forums.
This cross-domain evaluation reveals whether the distinctions learned on the
drugs domain generalize directly to the forums domain.
\end{itemize}  

\subsection{Results}

The accuracy scores for the different classifiers and settings are reported in Table~\ref{tab:results_drugs}.

\paragraph{Legal vs. illegal forums.}

The classification scores on the Onion forums data are reported in Table~\ref{tab:results_forums}.
Results when training and testing on forums data are much worse for the neural-based systems,
probably due to the fact that this data is much noisier and more varied,
containing different distributions of words in the training and testing
subsets. However, the SVM model achieves an accuracy of 85.3 in the full setting. The high results obtained by the model in the cases where the content words are dropped (drop.cont.) or replaced by part-of-speech tags (pos cont.) further the importance of the syntactic structure to the legal/illegal differentiation in this case. 

\paragraph{Cross-domain evaluation.}

Surprisingly, training on drugs data and evaluating on forums performs much
better than %both previous experiments
in the in-domain setting
for four out of five classifiers,
showing that while the forums data is noisy, it can be accurately classified
when training on the cleaner drugs data. This also shows that illegal texts in Tor share common properties regardless of topical category.
The lower results obtained by the Naive Bayes classifier in the drop cont. setting suggest that some of these properties are lexical.


\begin{figure*}[t]
\centering
\small
\begin{tabular}{l|l}
\multicolumn{1}{c}{\textbf{Legal Onion}} &
\multicolumn{1}{c}{\textbf{Illegal Onion}}\\
Generic Viagra Oral Jelly is used for Erectile Dys&8/03/2017 - ATTN! Looks like SIGAINT email provid\\
Fortis Testosteron Testosterone Enanthate 250mgml &Medical Grade Cannabis Buds We stock high quality \\
Generic Cialis is used to treat erection problems &1 BTC 2630.8 USD\\
2 Kits Misoprostol 200mg with 4 Tablets \$75 \$150&Cialis(r) is in a class of drugs called Phosphodie\\
Boldenone 300 New Boldenone 300 Olymp Labs&TZOLKIN CALENDAR, 140ug LSD Marquis PreTest\\
(generic Viagra) unbranded generic Sildenafil citr&Formed in early 2016, OzDrugsExpress is the work o\\
Mesterolone New Mesterolone Olymp Labs&500mg x 30 pills Price:$ 45.95 Per pill:$ 1.53 Ord\\
manufactured by: Cipla Pharmaceuticals Compare to &Generic Kaletra contains a combination of lopinavi\\
(generic Zoloft) Sertraline 25 mg tablets US\$31.05&/ roduct-category/cannabis/ Cannabis / 14g Amnesia\\
Sustanon-250mg Testosterone Compound 250mgml&Welcome to SnowKings Good Quality Cocaine
\end{tabular}
\caption{Example paragraphs (data instances) from the Legal Onion and
Onion Illegal subsets of the Onion corpus (ten examples from each).
Each paragraph is trimmed to the first 50 characters for space reasons.
\label{fig:examples}}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

As shown in \S\ref{sec:domain}, the Legal Onion and Illegal Onion domains
are quite distant in terms of word distribution and named entity wikification.
Moreover, named entity recognition and wikification work less well
for the illegal domain.
This has practical implications: we need to adapt our tools to deal with illegal Tor data.
As evident from the classification experiments (\S\ref{sec:classification}), however,
the Legal Onion and Onion Illegal texts are harder to distinguish than
eBay and Legal Onion, meaning we need to look beyond word distribution
when comparing these domains.

\paragraph{Analysis of texts from the datasets.}

Looking at specific sentences (Figure~\ref{fig:examples})
reveals that Legal Onion and Onion Illegal are easy to distinguish
based on the identity of certain words, e.g., terms for legal and illegal drugs,
respectively.
Thus looking at the wordforms is already a good solution for tackling this
classification problem,
and using modern text classification does not present an advantage in
terms of accuracy.

\begin{figure*}[t]
\centering
\small
\begin{tabular}{l|l}
\multicolumn{1}{c}{\textbf{Legal Onion}} &
\multicolumn{1}{c}{\textbf{Illegal Onion}}\\
( ADJ PROPN ) PROPN PROPN VERB NUM &3. We VERB VERB NOUN with ADV the A\\
( ADJ PROPN , PROPN ) PROPN NUM NOU&NOUN . NOUN NOUN PROPN NOUN\\
PROPN PROPN VERB PROPN NUM NOUN US\$&VERB NOUN NOUN : No ADJ than NUM NO\\
PROPN PROPN PROPN NUM NOUN \$ NUM PR&Welcome! We VERB ADJ to VERB a ADJ \\
NOUN NUM PROPN with NOUN - NOUN NUM&PROPN PROPN PROPN NOUN : ADJ NOUN V\\
PROPN NUM PROPN PROPN \$ NUM&( PROPN ) NUM PROPN PROPN NUM PROPN\\
ADJ PROPN NUM PROPN VERB US\$ NUM fo&NOUN PROPN NUM , NUM ADJ NOUN , ADJ\\
( ADJ PROPN ) NOUN NUM NOUN NOUN . &You VERB ADV VERB NUM of these NOUN\\
PROPN PROPN PROPN NUM PROPN PROPN -&/ NOUN - NOUN / NOUN PROPN / PROPN \\
PROPN / PROPN PROPN PROPN / PROPN P&Any NOUN VERB us.\\
\end{tabular}
\caption{Example paragraphs (data instances) from the Legal Onion and
Onion Illegal subsets of the Onion corpus (ten examples from each),
where content words are replaced with their parts of speech.
Each paragraph is trimmed to the first 50 characters for space reasons.
Different instances are shown than in Figure~\ref{fig:examples}.
\label{fig:examples_poscontent}}
\end{figure*}

\paragraph{Analysis of manipulated texts.}

Given that replacing content words with their POS tags significantly drops
performance for classification of legal vs illegal texts
(see ``pos cont.'' in \S\ref{sec:classification}),
we conclude that the distrubtion of parts of speech is not significantly
different between the domains,
and most differences can be attributed to content.
However, non-linear models such as the SVM or the order-aware neural models
do manage to distinguish between the texts even in this setting.
Indeed, Figure~\ref{fig:examples_poscontent} demonstrates that
there are easily identifiable patterns distinguishing between the domains,
but a bag-of-words approach is not strong enough to identify them.

On a methological note, Legal Onion and Illegal Onion are distinct enough
to be considered different domains (as distant as Legal Onion and eBay,
see \S\ref{sec:domain}). 
Therefore, Tor could be a good testbed for working on classification of
legal and illegal texts.\daniel{Similar to a sentence from before, not sure
I follow the logic}

\paragraph{Analysis of learned feature weights.}

Table~\ref{tab:nb_weights} shows the most indicative features learned
by the NB classifier (without input manipulation) for the Onion
legal vs. illegal setting.
Many strong features are function words, showing that the language itself
is quite different between the domains.

\begin{table}[t]
\small
\begin{tabular}{lr}
\multicolumn{2}{c}{\textbf{Legal Onion}}\\
feature & ratio\\
\hline
cart & 0.037\\
2016 & 0.063\\
Bh & 0.067\\
drugs & 0.067\\
EUR & 0.077\\
very & 0.083\\
Per & 0.091\\
delivery & 0.091\\
symptoms & 0.091\\
Please & 0.100\\
Quantity & 0.100\\
here & 0.100\\
check & 0.100\\
contact & 0.100\\
called & 0.100\\
not & 0.102\\
if & 0.105\\
If & 0.111\\
taking & 0.111\\
like & 0.111\\
questions & 0.111\\
Cart & 0.118\\
> & 0.125\\
take & 0.125\\
\end{tabular}
\begin{tabular}{lr}
\multicolumn{2}{c}{\textbf{Illegal Onion}}\\
feature & ratio\\
\hline
Laboratories & 8.500\\
Cipla & 9.000\\
Pfizer & 9.000\\
each & 9.571\\
Force & 10.000\\
Pharma & 10.333\\
grams & 10.333\\
300 & 10.500\\
Bitcoin & 11.500\\
RSM & 12.000\\
Sun & 12.000\\
manufactured & 12.667\\
Tablets & 13.000\\
tablets & 13.714\\
120 & 16.000\\
Moldova & 17.000\\
citrate & 18.000\\
Pharmaceuticals & 31.500\\
New & 33.000\\
200 & 36.000\\
blister & 48.000\\
generic & 49.500\\
60 & 55.000\\
@ & 63.000\\
\end{tabular}
\caption{Most indicative features for each class in NB classifier
trained on Onion legal vs. illegal drugs,
and the ratio between their number of occurrences in the ``illegal''
class and the ``legal'' class.
Left: features with lowest ratio; right: features with highest ratio.
\label{tab:nb_weights}}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
 












\bibliography{acl2019}
\bibliographystyle{acl_natbib}




\end{document}
