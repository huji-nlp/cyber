%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{The Language of Legal and Illegal Activity on DarkNet}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Omri

-- darknet has much illegal activity (https://www.profwoodward.org/2016/02/how-much-of-tor-is-used-for-illegal.html). people use it because it's easier to be anonymous, hard to track etc.

-- scalably monitoring activity in darknet uses NLP tools, but little is known about what characteristics the text in Tor has, and how well do off-the-shelf NLP tools do on this domain.

-- X et al. (2017) published a corpus and looked into text classification on texts from Tor, and Y et al. (2019) looked into how influential criminal sites are. 

-- We investigate how legal and illegal activity taken from DarkNet is different, comparing to a clearnet website with similar content as a control condition.

-- We also explore methods for classifying texts from DarkNet into legal and illegal. This is important both for understanding whether these two types are different in terms of
their text and in what ways, and as a practical tool.

-- A bit on the experiments and results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

Elior

\paragraph{Detecting illegal activities in the Web}

The detection of illegal activities in the Web is sometimes derived from a more general topic classification. For example, \citet{Biryukov14} used the software Mallet \citep{McCallum02} and the web service uClassify \citep{Kagstrom13} for a classification of the content of Tor hidden services into 18 categories, which allows the distinction between illegal or contreversial content on one hand and human rights or freedom of speech content on the other hand. \citet{GraczykKinningham15} combined unsupervised feature selection and an SVM classfier for the classification of drug sales in an anonymous marketplace. However, although the detection of illegal activities can be easily deduced in some cases, the legal status of a given product can change \citep{GraczykKinningham15} and a given topic could cover both legal and illegal content \citep{AlNabki17,Avarikioti18}.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datasets}

DUTA \citep{AlNabki17}

\paragraph{Cleaning.} Elior + Daniel

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Domain Analysis}

\subsection{Distances between Domains}

Leshem



\subsection{NER and Wikification}

In order to analyze the named entities in each domain and the differences
between them we used a ``wikification'' technique, searching for
said entities in public datasets such as Wikipedia. 

Using SpaCy's named entity recognition, we first found all the named
entities in each site in the datasets. After finding the named entites
we searched for relevant Wikipedia entries for each named entity using
the DBpedia Ontology API. For each domain we counted the total number
of named entites and what percentage of them had corresponding Wikipedia
articles.

\begin{table}

\caption{Percentage of Wikifiable Named Entites per Domain}

\begin{centering}
\begin{tabular}{|c||c|}
\hline 
 & Percentage\tabularnewline
\hline 
\hline 
Legal Onion & 50.8 $\pm2.31$\tabularnewline
\hline 
\hline 
Illegal Onion & 32.5 $\pm1.35$\tabularnewline
\hline 
\hline 
eBay & 38.6 $\pm2$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}
According to our results (Table 1) the wikification percentages of
eBay sites and illegal Onion sites are comparable and relatively low.
However, sites selling legal drugs on Onion have a much higher wikification
percentage.

Presumably the named entities in Onion sites selling legal drugs are
more easily found in public databases such as Wikipedia because they
are mainly well known names for legal pharmaceuticals. However, in
both illegal Onion and eBay sites, the list of named entities include
many nicknames for illicit drugs and paraphernalia. These nicknames
are usually not well known by the general public and are therefore
less likely to be found on Wikipedia or other public databases.

In addition to the differences in wikification percentages between
the domains, we found that SpaCy had trouble correctly identifying
named entities in both Onion and eBay sites. There were a fair number
of false positives (words and phrases that were found by SpaCy but
were not actually named entities), especially in illegal Onion sites.
We believe the informal language used in our datasets makes it harder
for SpaCy to function optimally in this capacity.

These findings lead us to believe that the popular tools used today
for named entity recognition and analysis are not ideal for processing
informal language on the internet, especially language dealing in
illicit activities. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classification Experiments}

Daniel

\subsection{Results}

\paragraph{Legal vs. Illegal}

\paragraph{Legal Onion vs. Ebay}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

The legal and illegal are pretty distant, which is evident in a few ways: word distribution is different, NER and Wikification work less well
for illegal. This has practical implications: we need to adapt our tools to deal with illegal Tor data. 

Looking at specific sentences, we see that it's hard distinguishing them based on the identity of the words, which means that
looking at the wordforms is a very poor solution for tackling this. However, using modern text classification, they can be distinguished
in a 72\%. 

Looking at how different types of language influence results: given that replacing all words with their POS tags gives
the same performance, this tells us their syntax is different as well.

Methological: Legal and illegal in Onion are distinct enough to be considered different domains (as distant as Legal and Ebay). 
Therefore, Tor could be a good testbed for working on legal and illegal classification.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
 












\bibliography{acl2019}
\bibliographystyle{acl_natbib}




\end{document}
